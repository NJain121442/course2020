{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# path_to_ds = 'Data/LessData/training_data/training_data/'\n",
    "# path_to_test_ds = 'Data/LessData/test_data/'\n",
    "path_to_ds = 'Data/training_data/training_data/'\n",
    "path_to_test_ds = 'Data/test_data/'\n",
    "node_hidden1 = 12\n",
    "edge_hidden1 = 12\n",
    "num_hops = 15\n",
    "batch_size = 400\n",
    "# pos_weights = torch.tensor([[22]]).to(torch.device('cuda'))\n",
    "# pos_weights = torch.tensor([[7.35]]).to(torch.device('cuda'))\n",
    "pos_weights = torch.tensor([[7.5]]).to(torch.device('cuda'))\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        filelist = glob.glob(path+'/*.json')\n",
    "        self.graphs = []\n",
    "        for fname in tqdm(filelist):\n",
    "            with open(fname) as jfile:\n",
    "                graph = nx.node_link_graph(json.load(jfile))\n",
    "                g = dgl.DGLGraph()\n",
    "                g.from_networkx(graph,node_attrs=['node_features'],edge_attrs=['distance','on_path'])        \n",
    "                self.graphs.append(g)\n",
    "        \n",
    "    def __len__(self):      \n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):    \n",
    "        return self.graphs[idx], self.graphs[idx].edata['on_path']\n",
    "    \n",
    "    \n",
    "train_ds = CustomDataset(path_to_ds)\n",
    "test_ds = CustomDataset(path_to_test_ds)\n",
    "\n",
    "def collate(samples):\n",
    "    # The input `samples` is a list, a batch of whatever comes out of your dataset object   \n",
    "    graphs = [x[0] for x in samples]\n",
    "    labels = [x[1] for x in samples]\n",
    "    batched_graph = dgl.batch(graphs,node_attrs=['node_features'],edge_attrs=['distance'])\n",
    "    targets = torch.cat(labels)    \n",
    "    return batched_graph, targets.unsqueeze(1).float()\n",
    "\n",
    "\n",
    "\n",
    "#this function is the edge update function - \n",
    "\n",
    "class EdgeNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EdgeNetwork, self).__init__()\n",
    "        in_features = 2*(2+node_hidden1)+1\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(in_features,edge_hidden1-2),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(edge_hidden1-2,edge_hidden1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #your input x is an object with the following properties:\n",
    "        #x.dst['node_features'], x.dst['node_hidden_state']\n",
    "        #x.src['node_features'], x.src['node_hidden_state']\n",
    "        #x.data['distance']\n",
    "        \n",
    "        #put them together with torch.cat\n",
    "        \n",
    "        #use a neural network to create an edge hidden represetation - \n",
    "        \n",
    "        #you return a dictionary with what you want to \"send\" to the reciving node\n",
    "        \n",
    "#         print(\"dst: \", x.dst['node_features'].shape)\n",
    "#         print(\"dst_h: \", x.dst['node_hidden_state'].shape)\n",
    "#         print(\"src: \", x.src['node_features'].shape)\n",
    "#         print(\"src_h: \", x.src['node_hidden_state'].shape)\n",
    "#         print(\"distance: \", torch.unsqueeze(x.data['distance'],1).shape)        \n",
    "        x_cat = torch.cat((x.dst['node_features'], \n",
    "                  x.dst['node_hidden_state'], \n",
    "                  x.src['node_features'], \n",
    "                  x.src['node_hidden_state'],\n",
    "                  torch.unsqueeze(x.data['distance'],1)), 1)\n",
    "#         print(\"here3: \", x_cat.shape)\n",
    "        output = self.layer1(x_cat)\n",
    "#         print(\"here4 - output: \", output.shape)\n",
    "        return {'edge hidden represetation': output }\n",
    "\n",
    "    \n",
    "class NodeNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NodeNetwork, self).__init__()\n",
    "        in_features = 2+edge_hidden1+node_hidden1\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(in_features,node_hidden1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "#             nn.Linear(node_hidden1-2,node_hidden1),\n",
    "#             nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #this time your input x has:\n",
    "        # x.mailbox['edge hidden represetation'] -> this is what you send with the edge update function above - \n",
    "        # it will have the size of the node neighborhood - \n",
    "        # (Batch size, number of nodes in neighborhood, edge hidden rep size), so you need to sum/mean over dim=1 \n",
    "        # x.data['node_hidden_state'] and x.data['node_features'] (this is the existing state of your node)\n",
    "        # you need to torch.cat the message sum, node hidden state, and node features \n",
    "        #- and then apply some fully connected neural network\n",
    "\n",
    "#         print(\"malibox: \", x.mailbox['edge hidden represetation'].sum(1).shape)\n",
    "#         print(\"data: \", x.data['node_features'].shape)\n",
    "#         print(\"data_h: \", x.data['node_hidden_state'].shape)\n",
    " \n",
    "        x_cat = torch.cat((\n",
    "            x.mailbox['edge hidden represetation'].sum(1),\n",
    "            x.data['node_features'],\n",
    "            x.data['node_hidden_state']\n",
    "        ), 1)\n",
    "        \n",
    "#         print(\"here1: \", x_cat.shape)\n",
    "        # return a new hidden state for the node\n",
    "        out = self.layer1(x_cat)\n",
    "#         print(\"here2 - out: \", out.shape)\n",
    "        return {'node_hidden_state': out }\n",
    "\n",
    "\n",
    "class EdgeClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EdgeClassifier, self).__init__()\n",
    "        in_features = 2*(2+node_hidden1)+1\n",
    "        self.layer1 = nn.Sequential(\n",
    "#             nn.Dropout(0.1),\n",
    "            nn.Linear(in_features,in_features),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(in_features,in_features),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features,1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(4,1),\n",
    "#             nn.LeakyReLU(0.5),\n",
    "#             nn.Sigmoid()\n",
    "#             nn.ReLU6()\n",
    "#             nn.Tanh()\n",
    "        )\n",
    "       \n",
    "    def forward(self, x):\n",
    "\n",
    "#         print(\"dst: \", x.dst['node_features'].shape)\n",
    "#         print(\"dst_h: \", x.dst['node_hidden_state'].shape)\n",
    "#         print(\"src: \", x.src['node_features'].shape)\n",
    "#         print(\"src_h: \", x.src['node_hidden_state'].shape)\n",
    "#         print(\"distance: \", torch.unsqueeze(x.data['distance'],1).shape)        \n",
    "        \n",
    "        x_cat = torch.cat((x.dst['node_features'], \n",
    "                  x.dst['node_hidden_state'], \n",
    "                  x.src['node_features'], \n",
    "                  x.src['node_hidden_state'],\n",
    "                  torch.unsqueeze(x.data['distance'],1)), 1)\n",
    "#         print(x_cat.shape)\n",
    "        out = self.layer1(x_cat)#torch.round(self.layer1(x_cat))\n",
    "        return {'edge_class_prediction': out }\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        # you need to create a network that \n",
    "        # will initialize your node hidden state based only on the node features -\n",
    "        self.node_init = nn.Sequential(\n",
    "            nn.Linear(2,node_hidden1),\n",
    "#             nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.edge_network = EdgeNetwork()\n",
    "        self.node_network = NodeNetwork()\n",
    "        \n",
    "        #this edge classifier is also an edge update function - \n",
    "        #but it needs to return something of size 1 (the edge class prediction)\n",
    "        #so either create a different model for this, or make the EdgeNetwork configurable\n",
    "        self.edge_classifier = EdgeClassifier()\n",
    "        \n",
    "    def forward(self, g):\n",
    "        \n",
    "        g.ndata['node_hidden_state'] = self.node_init(g.ndata['node_features'])\n",
    "        \n",
    "        for i in range(num_hops):\n",
    "#             print(i, \": \", g.ndata['node_hidden_state'].shape)\n",
    "            g.update_all(self.edge_network,self.node_network)\n",
    "            \n",
    "        #we want to classify the edges - so finally apply your edge classifier -\n",
    "#         print(\"here\")\n",
    "        g.apply_edges(self.edge_classifier)\n",
    "        \n",
    "        #and extract its output \n",
    "        out = g.edata['edge_class_prediction']\n",
    "        return out\n",
    "    \n",
    "\n",
    "net = Classifier()\n",
    "net = net.to(torch.device('cuda'))\n",
    "data_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_data_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=1e-2)\n",
    "# optimizer = optim.Adam(net.parameters(),lr=0.005,betas=(0.9, 0.999),eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 10, gamma=0.1)\n",
    "# loss_func = nn.BCEWithLogitsLoss(pos_weight=pos_weights, reduction='mean')\n",
    "loss_func = nn.BCEWithLogitsLoss(pos_weight=pos_weights, reduction='mean')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(40):\n",
    "\n",
    "    since = time.time()\n",
    "    val_loss_history = []\n",
    "    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "    best_loss = 10000.0\n",
    "    best_loss_TP = 0\n",
    "    best_loss_TN = 0\n",
    "    best_loss_FP = 0\n",
    "    best_loss_FN = 0\n",
    "\n",
    "    net.train()\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    true_negative = 0\n",
    "    false_negative = 0\n",
    "    total_positives = 0\n",
    "    total_negatives = 0\n",
    "    epoch_loss = 0\n",
    "    for x,y in data_loader:\n",
    "        x = x.to(torch.device('cuda'))\n",
    "        y = y.cuda()\n",
    "        predicted = net(x)\n",
    "#         batch_false_positive = (predicted!=y) & (y==0) )[0])\n",
    "#         batch_false_negative = len(np.where( (predicted_values!=y_values) & (y_values==1) )[0])\n",
    "        batch_loss = loss_func(predicted, y) #+ weighted_false_positive +  weighted_false_negative\n",
    "        epoch_loss += batch_loss.detach().item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            batch_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "#         with torch.no_grad():\n",
    "#             for param in net.parameters():\n",
    "#                 param.clamp_(-1, 1)   \n",
    "                \n",
    "        y_values = y.cpu().data.numpy()\n",
    "#         predicted_values = torch.round(predicted.cpu()).data.numpy()\n",
    "        predicted_values = nn.Sigmoid()(predicted.cpu()).data.numpy()\n",
    "        predicted_values[predicted_values >= 0.5] = 1\n",
    "        predicted_values[predicted_values <0.5] = 0\n",
    "        total_positives+=len(np.where( y_values==1 )[0])\n",
    "        total_negatives+=len(np.where( y_values==0 )[0])\n",
    "        true_positive+= len(np.where( (predicted_values==y_values) & (y_values==1) )[0])\n",
    "        true_negative+= len(np.where( (predicted_values==y_values) & (y_values==0) )[0])\n",
    "        false_positive+= len(np.where( (predicted_values!=y_values) & (y_values==0) )[0])\n",
    "        false_negative+= len(np.where( (predicted_values!=y_values) & (y_values==1) )[0])\n",
    "\n",
    "#     sample_data = predicted.cpu().data.numpy()\n",
    "#     sample_y = y.cpu().data.numpy()\n",
    "\n",
    "    TP = true_positive/total_positives\n",
    "    FN = false_negative/total_positives\n",
    "    TN = true_negative/total_negatives\n",
    "    FP = false_positive/total_negatives\n",
    "    print(\"Epoch \", epoch)\n",
    "    sample_data = torch.cat((predicted,nn.Sigmoid()(predicted), torch.round(nn.Sigmoid()(predicted)),y),1)\n",
    "    print(\"   Sample data (pred  |  Sigmoid(pred)  |  1/0 pred  |  y): \")\n",
    "    print(\"  \", sample_data)\n",
    "    print(\"   Training:   loss: {:.6f}  |  TP: {:.6f} | TN: {:.6f} | FP: {:.6f} | FN: {:.6f}\".format(\n",
    "                epoch_loss, TP, TN, FP, FN))\n",
    "#     print(\"               TP: \", true_positive, false_negative)\n",
    "    \n",
    "    net.eval()\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    true_negative = 0\n",
    "    false_negative = 0\n",
    "    total_positives = 0\n",
    "    total_negatives = 0\n",
    "    epoch_val_loss = 0\n",
    "    for x,y in test_data_loader:\n",
    "        x = x.to(torch.device('cuda'))\n",
    "        y = y.cuda()\n",
    "        predicted = net(x)\n",
    "        batch_val_loss = loss_func(predicted, y)\n",
    "        epoch_val_loss += batch_loss.detach().item()\n",
    "        y_values = y.cpu().data.numpy()\n",
    "#         predicted_values = torch.round(predicted.cpu()).data.numpy()\n",
    "#         predicted_values = predicted.cpu().data.numpy()\n",
    "        predicted_values = nn.Sigmoid()(predicted.cpu()).data.numpy()\n",
    "        predicted_values[predicted_values >= 0.5] = 1\n",
    "        predicted_values[predicted_values <0.5] = 0\n",
    "        total_positives+=len(np.where( y_values==1 )[0])\n",
    "        total_negatives+=len(np.where( y_values==0 )[0])\n",
    "        true_positive+= len(np.where( (predicted_values==y_values) & (y_values==1) )[0])\n",
    "        true_negative+= len(np.where( (predicted_values==y_values) & (y_values==0) )[0])\n",
    "        false_positive+= len(np.where( (predicted_values!=y_values) & (y_values==0) )[0])\n",
    "        false_negative+= len(np.where( (predicted_values!=y_values) & (y_values==1) )[0])\n",
    "    TP = true_positive/total_positives\n",
    "    FN = false_negative/total_positives\n",
    "    TN = true_negative/total_negatives\n",
    "    FP = false_positive/total_negatives\n",
    "    print(\"   Validation: loss: {:.6f}  |  TP: {:.6f} | TN: {:.6f} | FP: {:.6f} | FN: {:.6f}\".format(\n",
    "                epoch_val_loss, TP, TN, FP, FN))\n",
    "\n",
    "    # deep copy the model\n",
    "    if epoch_val_loss < best_loss:\n",
    "        best_loss = epoch_val_loss\n",
    "        best_model_wts = copy.deepcopy(net.state_dict())\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "        best_loss_TP = TP\n",
    "        best_loss_TN = TN\n",
    "        best_loss_FP = FP\n",
    "        best_loss_FN = FN\n",
    "\n",
    "print()\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "print('Best val Loss: {:4f}'.format(best_loss))\n",
    "print('With TP: {:.6f}% | TN: {:.6f}% | FP: {:.6f}% | FN: {:.6f}%'.format(\n",
    "                best_loss_TP*100, best_loss_TN*100, best_loss_FP*100, best_loss_FN*100))\n",
    "\n",
    "# load best model weights\n",
    "net.load_state_dict(best_model_wts)\n",
    "\n",
    "torch.save(net.state_dict(), 'hw3_graphs_final.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
